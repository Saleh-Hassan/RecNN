{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Determenistic Policy Gradients\n",
    "Work in progress nut it will be finished soon enough\n",
    "\n",
    "Special thanks to KnightofK9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "\n",
    "cuda = torch.device('cuda')\n",
    "frame_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('../data/ml-20m/ratings.csv')\n",
    "movies = pickle.load(open('../data/infos_pca128.pytorch', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credits: KnightofK9\n",
    "ratings[\"rating\"] = ratings[\"rating\"].apply(lambda i: 2 * (i - 2.5))\n",
    "users = ratings[[\"userId\",\"movieId\"]].groupby([\"userId\"]).size()\n",
    "users = users[users >= frame_size + 1]\n",
    "ratings = ratings[ratings[\"userId\"].isin(users.index)]\n",
    "ratings = ratings.sort_values(by=[\"userId\", \"timestamp\"]).drop(columns=[\"timestamp\"]).set_index(\"userId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in movies.keys():\n",
    "    movies[i] = movies[i].to(cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateRepresentation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StateRepresentation, self).__init__()\n",
    "        self.lin = nn.Sequential(\n",
    "            # 128 - embed size, 1 - rating size\n",
    "            nn.Linear(frame_size * (128 + 1), 256),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, info, ratings):\n",
    "        # raw_size - size of the raw movie info. Constant = 2591\n",
    "        # embed_size - size of an ebedded movie. Constant = 64\n",
    "        # raw -> embed via embeddings module defined above\n",
    "        # input: currently info is batch_size x frame_size x raw_size\n",
    "        # step 1: tramsform info to batch_size x (frame_size * embed_size)\n",
    "        info = info.view(batch_size, frame_size * 128)\n",
    "        # step 2: stack info with ratings. stacked: batch_size x (embed_size + 1)\n",
    "        stacked = torch.cat([info, ratings], 1)\n",
    "        # step 3: apply state represemtation module\n",
    "        state = self.lin(stacked)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.state_rep = StateRepresentation()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, num_actions)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, info, rewards):\n",
    "        state = self.state_rep(info, rewards)\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.tanh(self.linear3(x))\n",
    "        return state, x\n",
    "    \n",
    "    def get_action(self, info, rewards):\n",
    "        state, action = self.forward(info, rewards)\n",
    "        return state, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        action = torch.squeeze(action)\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG Code\n",
    "[Creditals](https://github.com/higgsfield/RL-Adventure-2/blob/master/5.ddpg.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg_update(batch, \n",
    "           gamma = 0.99,\n",
    "           min_value=-5,\n",
    "           max_value=5,\n",
    "           soft_tau=1e-2):\n",
    "    \n",
    "    state, action, reward, next_state, done = batch\n",
    "    \n",
    "    reward     = reward.unsqueeze(1)\n",
    "    done       = done.unsqueeze(1)\n",
    "    \n",
    "    enc_state, current_action = policy_net(*state)\n",
    "    \n",
    "    policy_loss = value_net(enc_state, current_action)\n",
    "    policy_loss = -policy_loss.mean()\n",
    "\n",
    "    enc_next_state, next_action = target_policy_net(*next_state)\n",
    "    target_value   = target_value_net(enc_next_state, next_action.detach())\n",
    "    expected_value = reward + (1.0 - done) * gamma * target_value\n",
    "    expected_value = torch.clamp(expected_value, min_value, max_value)\n",
    "    \n",
    "    value = value_net(enc_state, action)\n",
    "    \n",
    "    value_loss = value_criterion(value, expected_value.detach())\n",
    "\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward(retain_graph=True)\n",
    "    policy_optimizer.step()\n",
    "\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward(retain_graph=True)\n",
    "    value_optimizer.step()\n",
    "\n",
    "    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "            )\n",
    "\n",
    "    for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "            )\n",
    "            \n",
    "    return value_loss.item(), policy_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_net  = Critic(256, 128, 320).to(cuda)\n",
    "policy_net = Actor(256, 128, 192).to(cuda)\n",
    "\n",
    "target_value_net  = Critic(256, 128, 320).to(cuda)\n",
    "target_policy_net = Actor(256,128, 192).to(cuda)\n",
    "\n",
    "for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "    \n",
    "    \n",
    "value_lr  = 10e-3\n",
    "policy_lr = 10e-4\n",
    "\n",
    "value_optimizer  = optim.Adam(value_net.parameters(),  lr=value_lr)\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=policy_lr)\n",
    "value_criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter('../runs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd8ae6ad40e496abd680c07030a28d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=138493), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "n_iter = 1\n",
    "\n",
    "def prepare_batch(batch):\n",
    "    watched_infos = []\n",
    "    watched_rating = []\n",
    "    chosen_movie = []\n",
    "    chosen_rating = []\n",
    "    next_infos = []\n",
    "    next_rating = []\n",
    "    done = []\n",
    "    \n",
    "    for b in batch:\n",
    "        watched_infos.append(b[0][0])\n",
    "        watched_rating.append(torch.from_numpy(b[0][1]))\n",
    "        chosen_movie.append(b[1])\n",
    "        chosen_rating.append(b[2])\n",
    "        next_infos.append(b[3][0])\n",
    "        next_rating.append(torch.from_numpy(b[3][1]))\n",
    "        done.append(b[4])\n",
    "                    \n",
    "    watched_infos = torch.stack(watched_infos).to(cuda)\n",
    "    watched_rating = torch.stack(watched_rating).float().to(cuda)\n",
    "    chosen_movie = torch.stack(chosen_movie).to(cuda)\n",
    "    chosen_rating = torch.tensor(chosen_rating).to(cuda)\n",
    "    next_infos = torch.stack(next_infos).to(cuda)\n",
    "    next_rating = torch.stack(next_rating).float().to(cuda)\n",
    "    done = torch.tensor(done).float().to(cuda)\n",
    "    \n",
    "    return (watched_infos, watched_rating), chosen_movie, chosen_rating, \\\n",
    "           (next_infos, next_rating), done\n",
    "\n",
    "\n",
    "batch_bar = tqdm(total=len(users))\n",
    "batch = []\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "for user, df in ratings.groupby(level=0):\n",
    "    batch_bar.update(1)\n",
    "    size = max(len(df) - frame_size, 0)\n",
    "    for idx in range(0, size):\n",
    "        \n",
    "        if np.random.rand() < 0.8:  # intake percents\n",
    "            continue\n",
    "            \n",
    "        user_ratings = df[idx:frame_size + idx + 1]\n",
    "        user_ratings = user_ratings[[\"movieId\", \"rating\"]].values\n",
    "\n",
    "        chosen_movie = user_ratings[:, 0][-1] \n",
    "        chosen_movie = movies[chosen_movie] # action\n",
    "        chosen_rating = user_ratings[:, 1][-1] # reward\n",
    "        films_watched = user_ratings[:, 0][:-1] \n",
    "        watched_rating = user_ratings[:, 1][:-1] # state\n",
    "        watched_infos = [movies[i] for i in films_watched] # state\n",
    "        watched_infos = torch.stack(watched_infos)\n",
    "        next_infos = torch.cat((watched_infos[1:], chosen_movie.unsqueeze(0)), 0)\n",
    "        next_rating = watched_rating[1:].tolist()\n",
    "        next_rating.append(chosen_rating)\n",
    "        next_rating = np.array(next_rating)\n",
    "\n",
    "        # state action reward next_state done\n",
    "        batch.append([(watched_infos, watched_rating), chosen_movie, chosen_rating,\n",
    "                       (next_infos, next_rating), idx + 1 == size])\n",
    "                \n",
    "        if len(batch) >= batch_size:\n",
    "            # train here\n",
    "            batch = prepare_batch(batch)\n",
    "            value_loss, policy_loss = ddpg_update(batch)\n",
    "            writer.add_scalar('loss/value', value_loss, n_iter)\n",
    "            writer.add_scalar('loss/policy', policy_loss, n_iter)\n",
    "            n_iter += 1\n",
    "            batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37241"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0', dtype=torch.uint8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies[1].abs() > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
